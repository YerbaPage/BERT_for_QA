{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/apex01cp37cp37mlinux-x86-64whl/apex-0.1-cp37-cp37m-linux_x86_64.whl","execution_count":6,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/apex01cp37cp37mlinux-x86-64whl/apex-0.1-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: apex\nSuccessfully installed apex-0.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":7,"outputs":[{"output_type":"stream","text":"/kaggle/input/models/models/models/bert/vocab.txt\n/kaggle/input/models/models/models/bert/config.json\n/kaggle/input/models/models/models/bert/pytorch_model.bin\n/kaggle/input/models/models/models/albert/config.json\n/kaggle/input/models/models/models/albert/pytorch_model.bin\n/kaggle/input/models/models/models/albert/spiece.model\n/kaggle/input/models/models/transformers/modeling_tf_t5.py\n/kaggle/input/models/models/transformers/tokenization_utils.py\n/kaggle/input/models/models/transformers/modeling_albert.py\n/kaggle/input/models/models/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/modeling_xlm.py\n/kaggle/input/models/models/transformers/modeling_camembert.py\n/kaggle/input/models/models/transformers/modeling_tf_auto.py\n/kaggle/input/models/models/transformers/modeling_distilbert.py\n/kaggle/input/models/models/transformers/modeling_roberta.py\n/kaggle/input/models/models/transformers/tokenization_xlnet.py\n/kaggle/input/models/models/transformers/configuration_utils.py\n/kaggle/input/models/models/transformers/tokenization_transfo_xl.py\n/kaggle/input/models/models/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/configuration_xlm_roberta.py\n/kaggle/input/models/models/transformers/tokenization_xlm.py\n/kaggle/input/models/models/transformers/modeling_tf_utils.py\n/kaggle/input/models/models/transformers/modeling_gpt2.py\n/kaggle/input/models/models/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/pipelines.py\n/kaggle/input/models/models/transformers/modeling_tf_bert.py\n/kaggle/input/models/models/transformers/modeling_transfo_xl_utilities.py\n/kaggle/input/models/models/transformers/modelcard.py\n/kaggle/input/models/models/transformers/configuration_xlm.py\n/kaggle/input/models/models/transformers/configuration_t5.py\n/kaggle/input/models/models/transformers/configuration_xlnet.py\n/kaggle/input/models/models/transformers/modeling_tf_ctrl.py\n/kaggle/input/models/models/transformers/tokenization_ctrl.py\n/kaggle/input/models/models/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/modeling_tf_xlm.py\n/kaggle/input/models/models/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/configuration_bert.py\n/kaggle/input/models/models/transformers/configuration_ctrl.py\n/kaggle/input/models/models/transformers/modeling_tf_gpt2.py\n/kaggle/input/models/models/transformers/modeling_tf_distilbert.py\n/kaggle/input/models/models/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py\n/kaggle/input/models/models/transformers/configuration_openai.py\n/kaggle/input/models/models/transformers/configuration_mmbt.py\n/kaggle/input/models/models/transformers/modeling_encoder_decoder.py\n/kaggle/input/models/models/transformers/modeling_tf_openai.py\n/kaggle/input/models/models/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/configuration_roberta.py\n/kaggle/input/models/models/transformers/convert_pytorch_checkpoint_to_tf2.py\n/kaggle/input/models/models/transformers/configuration_transfo_xl.py\n/kaggle/input/models/models/transformers/modeling_t5.py\n/kaggle/input/models/models/transformers/modeling_openai.py\n/kaggle/input/models/models/transformers/configuration_distilbert.py\n/kaggle/input/models/models/transformers/modeling_utils.py\n/kaggle/input/models/models/transformers/tokenization_openai.py\n/kaggle/input/models/models/transformers/modeling_tf_roberta.py\n/kaggle/input/models/models/transformers/modeling_xlm_roberta.py\n/kaggle/input/models/models/transformers/hf_api.py\n/kaggle/input/models/models/transformers/modeling_ctrl.py\n/kaggle/input/models/models/transformers/optimization.py\n/kaggle/input/models/models/transformers/modeling_tf_albert.py\n/kaggle/input/models/models/transformers/configuration_auto.py\n/kaggle/input/models/models/transformers/configuration_camembert.py\n/kaggle/input/models/models/transformers/modeling_tf_transfo_xl_utilities.py\n/kaggle/input/models/models/transformers/tokenization_xlm_roberta.py\n/kaggle/input/models/models/transformers/tokenization_auto.py\n/kaggle/input/models/models/transformers/tokenization_albert.py\n/kaggle/input/models/models/transformers/modeling_tf_xlnet.py\n/kaggle/input/models/models/transformers/modeling_transfo_xl.py\n/kaggle/input/models/models/transformers/tokenization_bert.py\n/kaggle/input/models/models/transformers/__init__.py\n/kaggle/input/models/models/transformers/modeling_tf_pytorch_utils.py\n/kaggle/input/models/models/transformers/tokenization_bert_japanese.py\n/kaggle/input/models/models/transformers/tokenization_distilbert.py\n/kaggle/input/models/models/transformers/tokenization_gpt2.py\n/kaggle/input/models/models/transformers/tokenization_camembert.py\n/kaggle/input/models/models/transformers/modeling_tf_transfo_xl.py\n/kaggle/input/models/models/transformers/configuration_gpt2.py\n/kaggle/input/models/models/transformers/tokenization_roberta.py\n/kaggle/input/models/models/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py\n/kaggle/input/models/models/transformers/configuration_albert.py\n/kaggle/input/models/models/transformers/modeling_bert.py\n/kaggle/input/models/models/transformers/optimization_tf.py\n/kaggle/input/models/models/transformers/modeling_mmbt.py\n/kaggle/input/models/models/transformers/tokenization_t5.py\n/kaggle/input/models/models/transformers/file_utils.py\n/kaggle/input/models/models/transformers/modeling_auto.py\n/kaggle/input/models/models/transformers/modeling_xlnet.py\n/kaggle/input/models/models/transformers/commands/serving.py\n/kaggle/input/models/models/transformers/commands/train.py\n/kaggle/input/models/models/transformers/commands/run.py\n/kaggle/input/models/models/transformers/commands/__init__.py\n/kaggle/input/models/models/transformers/commands/download.py\n/kaggle/input/models/models/transformers/commands/user.py\n/kaggle/input/models/models/transformers/commands/convert.py\n/kaggle/input/models/models/transformers/data/__init__.py\n/kaggle/input/models/models/transformers/data/processors/glue.py\n/kaggle/input/models/models/transformers/data/processors/__init__.py\n/kaggle/input/models/models/transformers/data/processors/squad.py\n/kaggle/input/models/models/transformers/data/processors/utils.py\n/kaggle/input/models/models/transformers/data/processors/xnli.py\n/kaggle/input/models/models/transformers/data/metrics/squad_metrics.py\n/kaggle/input/models/models/transformers/data/metrics/__init__.py\n/kaggle/input/tensorflow2-question-answering/sample_submission.csv\n/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\n/kaggle/input/apex01cp37cp37mlinux-x86-64whl/apex-0.1-cp37-cp37m-linux_x86_64.whl\n/kaggle/input/apex01cp37cp37mlinux-x86-64whl/requirements.txt\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip list","execution_count":8,"outputs":[{"output_type":"stream","text":"Package                            Version             Location\r\n---------------------------------- ------------------- --------------\r\nabsl-py                            0.9.0\r\nadal                               1.2.2\r\naffine                             2.3.0\r\naiohttp                            3.6.2\r\nalabaster                          0.7.12\r\nalbumentations                     0.4.5\r\nalembic                            1.4.2\r\nallennlp                           0.9.0\r\naltair                             4.1.0\r\nanaconda-client                    1.7.2\r\nanaconda-project                   0.8.3\r\nannoy                              1.16.3\r\nansiwrap                           0.8.4\r\napex                               0.1\r\nappdirs                            1.4.3\r\nargh                               0.26.2\r\narrow                              0.15.5\r\narviz                              0.8.3\r\nasn1crypto                         1.3.0\r\nastroid                            2.3.3\r\nastropy                            4.0.1.post1\r\nastunparse                         1.6.3\r\nasync-generator                    1.10\r\nasync-timeout                      3.0.1\r\natomicwrites                       1.3.0\r\nattrs                              19.3.0\r\naudioread                          2.1.8\r\nautopep8                           1.5.1\r\nBabel                              2.8.0\r\nbackcall                           0.1.0\r\nbackports.shutil-get-terminal-size 1.0.0\r\nBaker                              1.3\r\nbasemap                            1.2.1\r\nbayesian-optimization              1.2.0\r\nbayespy                            0.5.19\r\nbcolz                              1.2.1\r\nbeautifulsoup4                     4.9.0\r\nbinaryornot                        0.4.4\r\nbiopython                          1.77\r\nbitarray                           1.2.1\r\nbkcharts                           0.2\r\nblack                              19.10b0\r\nbleach                             3.1.4\r\nblinker                            1.4\r\nblis                               0.4.1\r\nbokeh                              2.0.1\r\nBoruta                             0.3\r\nboto                               2.49.0\r\nboto3                              1.13.23\r\nbotocore                           1.16.23\r\nBottleneck                         1.3.2\r\nbq-helper                          0.4.1               /src/bq-helper\r\nbqplot                             0.12.12\r\nbranca                             0.4.1\r\nbrewer2mpl                         1.4.1\r\nbrotlipy                           0.7.0\r\ncachetools                         3.1.1\r\ncairocffi                          1.1.0\r\nCairoSVG                           2.4.2\r\nCartopy                            0.18.0\r\ncatalogue                          1.0.0\r\ncatalyst                           20.6\r\ncatboost                           0.23.2\r\ncategory-encoders                  2.2.2\r\ncertifi                            2020.4.5.1\r\ncesium                             0.9.12\r\ncffi                               1.14.0\r\ncftime                             1.1.3\r\nchainer                            7.4.0\r\nchainer-chemistry                  0.7.0\r\nchainercv                          0.13.1\r\nchardet                            3.0.4\r\ncleverhans                         3.0.1\r\nclick                              7.1.1\r\nclick-plugins                      1.1.1\r\ncliff                              3.1.0\r\ncligj                              0.5.0\r\ncloud-tpu-client                   0.8\r\ncloudpickle                        1.3.0\r\nclyent                             1.2.2\r\ncmaes                              0.5.0\r\ncmd2                               0.8.9\r\ncmdstanpy                          0.4.0\r\ncmudict                            0.4.4\r\ncolorama                           0.4.3\r\ncolorcet                           2.0.2\r\ncolorlog                           4.1.0\r\ncolorlover                         0.3.0\r\nconda                              4.8.3\r\nconda-package-handling             1.6.0\r\nConfigArgParse                     1.2.3\r\nconfigparser                       5.0.0\r\nconfuse                            1.1.0\r\nconllu                             1.3.1\r\ncontextily                         1.0.0\r\ncontextlib2                        0.6.0.post1\r\nconvertdate                        2.2.1\r\nconx                               3.7.10\r\ncookiecutter                       1.7.0\r\ncoverage                           5.1\r\ncrc32c                             2.0\r\ncryptography                       2.8\r\ncssselect2                         0.3.0\r\ncufflinks                          0.17.3\r\ncupy-cuda101                       7.5.0\r\nCVXcanon                           0.1.1\r\ncvxpy                              1.1.1\r\ncycler                             0.10.0\r\ncymem                              2.0.3\r\ncysignals                          1.10.2\r\nCython                             0.29.19\r\ncytoolz                            0.10.1\r\ndask                               2.17.2\r\ndask-glm                           0.2.0\r\ndask-ml                            1.5.0\r\ndask-xgboost                       0.1.10\r\ndatashader                         0.11.0\r\ndatashape                          0.5.2\r\ndeap                               1.3.1\r\ndecorator                          4.4.2\r\ndeepdish                           0.3.6\r\ndefusedxml                         0.6.0\r\nDelorean                           1.0.0\r\nDeprecated                         1.2.10\r\ndeprecation                        2.1.0\r\ndescartes                          1.1.0\r\ndiff-match-patch                   20181111\r\ndill                               0.3.1.1\r\ndipy                               1.1.1\r\ndistributed                        2.14.0\r\ndlib                               19.19.0\r\ndocker                             4.2.0\r\ndocker-pycreds                     0.4.0\r\ndocopt                             0.6.2\r\ndocutils                           0.15.2\r\nearthengine-api                    0.1.224\r\necos                               2.0.7.post1\r\neditdistance                       0.5.3\r\neli5                               0.10.1\r\nemoji                              0.5.4\r\nen-core-web-lg                     2.2.5\r\nen-core-web-sm                     2.2.5\r\nentrypoints                        0.3\r\nephem                              3.7.7.1\r\nessentia                           2.1b6.dev234\r\net-xmlfile                         1.0.1\r\nfancyimpute                        0.5.4\r\nfastai                             1.0.61\r\nfastcache                          1.1.0\r\nfastprogress                       0.2.3\r\nfastrlock                          0.4\r\nfasttext                           0.9.2\r\nfbpca                              1.0\r\nfbprophet                          0.6\r\nfeather-format                     0.4.1\r\nfeaturetools                       0.15.0\r\nfilelock                           3.0.10\r\nFiona                              1.8.13\r\nfitter                             1.2.1\r\nflake8                             3.7.9\r\nflaky                              3.6.1\r\nflashtext                          2.7\r\nFlask                              1.1.2\r\nFlask-Cors                         3.0.8\r\nfolium                             0.11.0\r\nfsspec                             0.7.2\r\nftfy                               5.7\r\nfuncy                              1.14\r\nfury                               0.5.1\r\nfuture                             0.18.2\r\nfuzzywuzzy                         0.18.0\r\ngast                               0.3.3\r\ngatspy                             0.3\r\ngcsfs                              0.6.1\r\nGDAL                               3.0.4\r\ngensim                             3.8.3\r\ngeographiclib                      1.50\r\nGeohash                            1.0\r\ngeojson                            2.5.0\r\ngeopandas                          0.6.3\r\ngeoplot                            0.4.1\r\ngeopy                              1.22.0\r\ngeoviews                           1.8.1\r\ngevent                             1.5.0\r\nggplot                             0.11.5\r\ngitdb                              4.0.4\r\nGitPython                          3.1.1\r\nglob2                              0.7\r\ngluoncv                            0.7.0\r\ngluonnlp                           0.9.1\r\ngmpy2                              2.1.0b1\r\ngoogle                             2.0.3\r\ngoogle-api-core                    1.17.0\r\ngoogle-api-python-client           1.8.0\r\ngoogle-auth                        1.14.0\r\ngoogle-auth-httplib2               0.0.3\r\ngoogle-auth-oauthlib               0.4.1\r\ngoogle-cloud-automl                0.10.0\r\ngoogle-cloud-bigquery              1.12.1\r\ngoogle-cloud-bigtable              1.2.1\r\ngoogle-cloud-core                  1.3.0\r\ngoogle-cloud-dataproc              0.7.0\r\ngoogle-cloud-datastore             1.12.0\r\ngoogle-cloud-firestore             1.6.2\r\ngoogle-cloud-kms                   1.4.0\r\ngoogle-cloud-language              1.3.0\r\ngoogle-cloud-logging               1.15.0\r\ngoogle-cloud-pubsub                1.4.3\r\ngoogle-cloud-scheduler             1.2.1\r\ngoogle-cloud-spanner               1.15.1\r\ngoogle-cloud-speech                1.3.2\r\ngoogle-cloud-storage               1.27.0\r\ngoogle-cloud-tasks                 1.5.0\r\ngoogle-cloud-translate             2.0.1\r\ngoogle-cloud-videointelligence     1.14.0\r\ngoogle-cloud-vision                1.0.0\r\ngoogle-pasta                       0.2.0\r\ngoogle-resumable-media             0.5.0\r\ngoogleapis-common-protos           1.51.0\r\ngplearn                            0.4.1\r\ngpxpy                              1.4.1\r\ngql                                0.2.0\r\ngraphql-core                       1.1\r\ngraphviz                           0.8.4\r\ngreenlet                           0.4.15\r\ngrpc-google-iam-v1                 0.12.3\r\ngrpcio                             1.29.0\r\ngrpcio-gcp                         0.2.2\r\ngym                                0.17.2\r\nh2o                                3.30.0.4\r\nh5py                               2.10.0\r\nhaversine                          2.2.0\r\nheamy                              0.0.7\r\nHeapDict                           1.0.1\r\nhep-ml                             0.6.1\r\nhmmlearn                           0.2.3\r\nholidays                           0.10.2\r\nholoviews                          1.13.2\r\nhpsklearn                          0.1.0\r\nhtml5lib                           1.0.1\r\nhtmlmin                            0.1.12\r\nhttplib2                           0.17.2\r\nhttplib2shim                       0.0.3\r\nhumanize                           2.4.0\r\nhunspell                           0.5.5\r\nhusl                               4.0.3\r\nhyperopt                           0.2.4\r\nhypertools                         0.6.2\r\nhypothesis                         5.10.0\r\nibis-framework                     1.3.0\r\nidna                               2.9\r\nImageHash                          4.1.0\r\nimageio                            2.8.0\r\nimagesize                          1.2.0\r\nimbalanced-learn                   0.6.2\r\nimgaug                             0.2.6\r\nimplicit                           0.4.2\r\nimportlib-metadata                 1.6.0\r\nintervaltree                       3.0.2\r\nipykernel                          5.1.1\r\nipython                            7.13.0\r\nipython-genutils                   0.2.0\r\nipython-sql                        0.3.9\r\nipywidgets                         7.5.1\r\niso3166                            1.0.1\r\nisort                              4.3.21\r\nisoweek                            1.3.3\r\nitsdangerous                       1.1.0\r\nJanome                             0.3.10\r\njax                                0.1.62\r\njaxlib                             0.1.41\r\njdcal                              1.4.1\r\njedi                               0.15.2\r\njeepney                            0.4.3\r\njieba                              0.42.1\r\nJinja2                             2.11.2\r\njinja2-time                        0.2.0\r\njmespath                           0.10.0\r\njoblib                             0.14.1\r\njson5                              0.9.0\r\njsonnet                            0.16.0\r\njsonpickle                         1.4.1\r\njsonschema                         3.2.0\r\njupyter                            1.0.0\r\njupyter-aihub-deploy-extension     0.1\r\njupyter-client                     6.1.3\r\njupyter-console                    6.1.0\r\njupyter-core                       4.6.3\r\njupyter-http-over-ws               0.0.8\r\njupyterlab                         1.2.10\r\njupyterlab-git                     0.10.0\r\njupyterlab-server                  1.1.1\r\nkaggle                             1.5.6\r\nkaggle-environments                0.3.12\r\nKeras                              2.3.1\r\nKeras-Applications                 1.0.8\r\nKeras-Preprocessing                1.1.2\r\nkeras-tuner                        1.0.1\r\nkeyring                            21.1.1\r\nkiwisolver                         1.2.0\r\nkmapper                            1.2.0\r\nkmeans-smote                       0.1.2\r\nkmodes                             0.10.2\r\nknnimpute                          0.1.0\r\nkorean-lunar-calendar              0.2.1\r\nkornia                             0.3.1\r\nkubernetes                         10.1.0\r\nlangid                             1.1.6\r\nLasagne                            0.2.dev1\r\nlazy-object-proxy                  1.4.3\r\nlearntools                         0.3.4\r\nleven                              1.0.4\r\nlibarchive-c                       2.9\r\nlibrosa                            0.7.2\r\nlief                               0.9.0\r\nlightfm                            1.15\r\nlightgbm                           2.3.1\r\nlime                               0.2.0.0\r\nline-profiler                      3.0.2\r\nllvmlite                           0.31.0\r\nlml                                0.0.9\r\nlocket                             0.2.0\r\nLunarCalendar                      0.0.9\r\nlxml                               4.5.0\r\nMako                               1.1.3\r\nmapclassify                        2.2.0\r\nmarisa-trie                        0.7.5\r\nMarkdown                           3.2.1\r\nmarkovify                          0.8.0\r\nMarkupSafe                         1.1.1\r\nmatplotlib                         3.2.1\r\nmatplotlib-venn                    0.11.5\r\nmccabe                             0.6.1\r\nmemory-profiler                    0.57.0\r\nmercantile                         1.1.4\r\nmissingno                          0.4.2\r\nmistune                            0.8.4\r\nmizani                             0.7.1\r\nmkl-fft                            1.1.0\r\nmkl-random                         1.1.0\r\nmkl-service                        2.3.0\r\nml-metrics                         0.1.4\r\nmlcrate                            0.2.0\r\nmlens                              0.2.3\r\nmlxtend                            0.17.2\r\nmmh3                               2.5.1\r\nmne                                0.20.5\r\nmnist                              0.2.2\r\nmock                               3.0.5\r\nmore-itertools                     8.2.0\r\nmpld3                              0.3\r\nmplleaflet                         0.0.5\r\nmpmath                             1.1.0\r\nmsgpack                            0.6.2\r\nmsgpack-numpy                      0.4.6.post0\r\nmultidict                          4.7.6\r\nmultipledispatch                   0.6.0\r\nmultiprocess                       0.70.9\r\nmunch                              2.5.0\r\nmurmurhash                         1.0.2\r\nmxnet-cu101                        1.6.0\r\nmypy-extensions                    0.4.3\r\nnb-conda                           2.2.1\r\nnb-conda-kernels                   2.2.3\r\nnbclient                           0.2.0\r\nnbconvert                          5.6.1\r\nnbdime                             2.0.0\r\nnbformat                           5.0.6\r\nnbpresent                          3.0.2\r\nnervananeon                        2.6.0\r\nnest-asyncio                       1.3.2\r\nnetCDF4                            1.5.3\r\nnetworkx                           2.4\r\nnibabel                            3.1.0\r\nnilearn                            0.6.2\r\nnltk                               3.2.4\r\nnnabla                             1.8.0\r\nnnabla-ext-cuda101                 1.8.0\r\nnolearn                            0.6.1\r\nnose                               1.3.7\r\nnotebook                           5.5.0\r\nnotebook-executor                  0.2\r\nnumba                              0.48.0\r\nnumexpr                            2.7.1\r\nnumpy                              1.18.1\r\nnumpydoc                           0.9.2\r\nnvidia-ml-py3                      7.352.0\r\noauth2client                       4.1.3\r\noauthlib                           3.0.1\r\nodfpy                              1.4.1\r\nolefile                            0.46\r\nonnx                               1.7.0\r\nopencv-python                      4.2.0.34\r\nopenpyxl                           3.0.3\r\nopenslide-python                   1.1.1\r\nopt-einsum                         3.2.1\r\noptuna                             1.5.0\r\norderedmultidict                   1.0.1\r\nortools                            7.6.7691\r\nosmnx                              0.14.0\r\nosqp                               0.6.1\r\noverrides                          3.0.0\r\nOWSLib                             0.19.2\r\npackaging                          20.1\r\npalettable                         3.3.0\r\npandas                             1.0.3\r\npandas-datareader                  0.8.1\r\npandas-profiling                   2.6.0\r\npandas-summary                     0.0.7\r\npandasql                           0.7.3\r\npandoc                             1.0.2\r\npandocfilters                      1.4.2\r\npanel                              0.9.5\r\npapermill                          2.1.0\r\nparam                              1.9.3\r\nparsimonious                       0.8.1\r\nparso                              0.5.2\r\npartd                              1.1.0\r\npath                               13.1.0\r\npath.py                            12.4.0\r\npathlib2                           2.3.5\r\npathos                             0.2.5\r\npathspec                           0.8.0\r\npathtools                          0.1.2\r\npatsy                              0.5.1\r\npbr                                5.4.5\r\npdf2image                          1.13.1\r\nPDPbox                             0.2.0+13.g73c6966\r\npep8                               1.7.1\r\npexpect                            4.8.0\r\nphik                               0.9.11\r\npickleshare                        0.7.5\r\nPillow                             5.4.1\r\npip                                20.1.1\r\npkginfo                            1.5.0.1\r\nplac                               0.9.6\r\nplotly                             4.8.1\r\nplotly-express                     0.4.1\r\nplotnine                           0.6.0\r\npluggy                             0.13.0\r\nply                                3.11\r\npolyglot                           16.7.4\r\nportalocker                        1.7.0\r\nposix-ipc                          1.0.4\r\npox                                0.2.7\r\npoyo                               0.5.0\r\nppca                               0.0.4\r\nppft                               1.6.6.1\r\npreprocessing                      0.1.13\r\npreshed                            3.0.2\r\nprettytable                        0.7.2\r\nprometheus-client                  0.7.1\r\npromise                            2.3\r\nprompt-toolkit                     3.0.5\r\npronouncing                        0.2.0\r\nprotobuf                           3.11.4\r\npsutil                             5.7.0\r\nptyprocess                         0.6.0\r\npudb                               2019.2\r\npy                                 1.8.1\r\npy-cpuinfo                         5.0.0\r\npy-lz4framed                       0.14.0\r\npy-spy                             0.3.3\r\npy-stringmatching                  0.4.1\r\npy-stringsimjoin                   0.3.1\r\npyahocorasick                      1.4.0\r\npyaml                              20.4.0\r\nPyArabic                           0.6.7\r\npyarrow                            0.16.0\r\npyasn1                             0.4.8\r\npyasn1-modules                     0.2.7\r\nPyAstronomy                        0.15.0\r\npybind11                           2.5.0\r\nPyBrain                            0.3\r\npycairo                            1.19.1\r\npycodestyle                        2.5.0\r\npycosat                            0.6.3\r\npycountry                          19.8.18\r\npycparser                          2.20\r\npycrypto                           2.6.1\r\npyct                               0.4.6\r\npycuda                             2019.1.2\r\npycurl                             7.43.0.5\r\npydash                             4.7.6\r\npydicom                            2.0.0\r\npydocstyle                         5.0.2\r\npydot                              1.4.1\r\npyemd                              0.5.1\r\npyepsg                             0.4.0\r\npyexcel-io                         0.5.20\r\npyexcel-ods                        0.5.6\r\npyfasttext                         0.4.6\r\npyflakes                           2.1.1\r\npyglet                             1.5.0\r\nPygments                           2.6.1\r\nPyJWT                              1.7.1\r\npykalman                           0.9.5\r\npyLDAvis                           2.1.2\r\npylint                             2.4.4\r\npymc3                              3.8\r\nPyMeeus                            0.3.7\r\npymongo                            3.10.1\r\nPympler                            0.8\r\npynvrtc                            9.2\r\npyocr                              0.7.2\r\npyodbc                             4.0.30\r\npyOpenSSL                          19.1.0\r\npypandoc                           1.5\r\npyparsing                          2.4.7\r\npyPdf                              1.13\r\npyperclip                          1.8.0\r\nPyPrind                            2.11.2\r\npyproj                             2.6.1.post1\r\nPyQt5                              5.12.3\r\nPyQt5-sip                          4.19.18\r\nPyQtWebEngine                      5.12.1\r\npyrsistent                         0.16.0\r\npysal                              2.1.0\r\npyshp                              2.1.0\r\nPySocks                            1.7.1\r\npystan                             2.19.1.1\r\npytagcloud                         0.3.5\r\npytesseract                        0.3.4\r\npytest                             5.4.1\r\npytest-arraydiff                   0.3\r\npytest-astropy                     0.7.0\r\npytest-astropy-header              0.1.2\r\npytest-cov                         2.9.0\r\npytest-doctestplus                 0.4.0\r\npytest-mock                        3.1.1\r\npytest-openfiles                   0.4.0\r\npytest-remotedata                  0.3.1\r\npytext-nlp                         0.1.2\r\npython-dateutil                    2.8.1\r\npython-editor                      1.0.4\r\npython-igraph                      0.8.2\r\npython-jsonrpc-server              0.3.4\r\npython-language-server             0.31.10\r\npython-Levenshtein                 0.12.0\r\npython-louvain                     0.14\r\npython-slugify                     4.0.0\r\npytools                            2020.2\r\npytorch-ignite                     0.3.0\r\npytorch-pretrained-bert            0.6.2\r\npytorch-transformers               1.1.0\r\npytz                               2019.3\r\nPyUpSet                            0.1.1.post7\r\npyviz-comms                        0.7.4\r\nPyWavelets                         1.1.1\r\npyxdg                              0.26\r\nPyYAML                             5.3.1\r\npyzmq                              19.0.0\r\nQDarkStyle                         2.8.1\r\nqgrid                              1.3.1\r\nQtAwesome                          0.7.1\r\nqtconsole                          4.7.3\r\nQtPy                               1.9.0\r\nrandomgen                          1.16.6\r\nrasterio                           1.1.5\r\nray                                0.8.5\r\nredis                              3.4.1\r\nregex                              2020.4.4\r\nrequests                           2.23.0\r\nrequests-oauthlib                  1.2.0\r\nresampy                            0.2.2\r\nresponses                          0.10.14\r\nretrying                           1.3.3\r\nrgf-python                         3.8.0\r\nrope                               0.16.0\r\nrsa                                4.0\r\nRtree                              0.9.4\r\nruamel-yaml                        0.15.80\r\ns2sphere                           0.2.5\r\ns3fs                               0.4.2\r\ns3transfer                         0.3.3\r\nsacred                             0.8.1\r\nsacremoses                         0.0.43\r\nscattertext                        0.0.2.64\r\nscikit-image                       0.16.2\r\nscikit-learn                       0.23.1\r\nscikit-multilearn                  0.2.0\r\nscikit-optimize                    0.7.4\r\nscikit-plot                        0.3.7\r\nscikit-surprise                    1.1.0\r\nscipy                              1.4.1\r\nscs                                2.1.2\r\nseaborn                            0.10.0\r\nSecretStorage                      3.1.2\r\nSend2Trash                         1.5.0\r\nsentencepiece                      0.1.91\r\nsentry-sdk                         0.14.4\r\nsetuptools                         46.1.3.post20200325\r\nsetuptools-git                     1.2\r\nshap                               0.35.0\r\nShapely                            1.7.0\r\nshortuuid                          1.0.1\r\nsimplegeneric                      0.8.1\r\nSimpleITK                          1.2.4\r\nsimplejson                         3.17.0\r\nsingledispatch                     3.4.0.3\r\nsip                                4.19.20\r\nsix                                1.14.0\r\nsklearn                            0.0\r\nsklearn-contrib-py-earth           0.1.0+1.gdde5f89\r\nsklearn-pandas                     1.8.0\r\nsmart-open                         2.0.0\r\nsmhasher                           0.150.1\r\nsmmap                              3.0.2\r\nsnowballstemmer                    2.0.0\r\nsnuggs                             1.4.7\r\nsortedcollections                  1.1.2\r\nsortedcontainers                   2.1.0\r\nSoundFile                          0.10.3.post1\r\nsoupsieve                          1.9.4\r\nspacy                              2.2.3\r\nspectral                           0.21\r\nSphinx                             3.0.2\r\nsphinx-rtd-theme                   0.2.4\r\nsphinxcontrib-applehelp            1.0.2\r\nsphinxcontrib-devhelp              1.0.2\r\nsphinxcontrib-htmlhelp             1.0.3\r\nsphinxcontrib-jsmath               1.0.1\r\nsphinxcontrib-qthelp               1.0.3\r\nsphinxcontrib-serializinghtml      1.1.4\r\nsphinxcontrib-websupport           1.2.1\r\nspyder                             4.1.2\r\nspyder-kernels                     1.9.0\r\nSQLAlchemy                         1.3.16\r\nsqlparse                           0.3.1\r\nsquarify                           0.4.3\r\nsrsly                              1.0.2\r\nstatsmodels                        0.11.1\r\nstemming                           1.0.1\r\nstevedore                          2.0.0\r\nstop-words                         2018.7.23\r\nstopit                             1.1.2\r\nsubprocess32                       3.5.4\r\nsvgwrite                           1.4\r\nsympy                              1.5.1\r\ntables                             3.6.1\r\ntabulate                           0.8.7\r\ntangled-up-in-unicode              0.0.4\r\ntblib                              1.6.0\r\ntenacity                           6.1.0\r\ntensorboard                        2.2.2\r\ntensorboard-plugin-wit             1.6.0.post3\r\ntensorboardX                       2.0\r\ntensorflow                         2.2.0\r\ntensorflow-addons                  0.10.0\r\ntensorflow-estimator               2.2.0\r\ntensorflow-gcs-config              2.1.7\r\ntensorflow-hub                     0.8.0\r\ntensorflow-probability             0.10.0\r\nTensorforce                        0.5.4\r\ntensorpack                         0.10.1\r\ntermcolor                          1.1.0\r\nterminado                          0.8.3\r\nterminalplot                       0.3.0\r\nterminaltables                     3.1.0\r\ntestpath                           0.4.4\r\ntext-unidecode                     1.3\r\ntextblob                           0.15.3\r\ntexttable                          1.6.2\r\ntextwrap3                          0.9.2\r\nTheano                             1.0.4\r\nthinc                              7.3.1\r\nthreadpoolctl                      2.1.0\r\ntifffile                           2020.6.3\r\ntinycss2                           1.0.2\r\ntokenizers                         0.7.0\r\ntoml                               0.10.0\r\ntoolz                              0.10.0\r\ntorch                              1.5.0\r\ntorchaudio                         0.5.0a0+3305d5c\r\ntorchtext                          0.6.0\r\ntorchvision                        0.6.0a0+82fd1c8\r\ntornado                            5.0.2\r\nTPOT                               0.11.5\r\ntqdm                               4.45.0\r\ntraitlets                          4.3.3\r\ntraittypes                         0.2.1\r\ntransformers                       2.11.0\r\ntrueskill                          0.4.5\r\ntsfresh                            0.16.0\r\ntyped-ast                          1.4.1\r\ntypeguard                          2.8.0\r\ntyping                             3.7.4.1\r\ntyping-extensions                  3.7.4.1\r\ntzlocal                            2.1\r\nujson                              1.35\r\numap-learn                         0.3.10\r\nunicodecsv                         0.14.1\r\nUnidecode                          1.1.1\r\nupdate-checker                     0.17\r\nuritemplate                        3.0.1\r\nurllib3                            1.24.3\r\nurwid                              2.1.0\r\nvecstack                           0.4.0\r\nvisions                            0.4.1\r\nvowpalwabbit                       8.8.1\r\nvtk                                8.1.2\r\nWand                               0.5.3\r\nwandb                              0.8.36\r\nwasabi                             0.6.0\r\nwatchdog                           0.10.2\r\nwavio                              0.0.4\r\nwcwidth                            0.1.9\r\nwebencodings                       0.5.1\r\nwebsocket-client                   0.57.0\r\nWerkzeug                           1.0.1\r\nwfdb                               2.2.1\r\nwheel                              0.34.2\r\nwhichcraft                         0.6.1\r\nwidgetsnbextension                 3.5.1\r\nword2number                        1.1\r\nWordbatch                          1.4.6\r\nwordcloud                          1.7.0\r\nwordsegment                        1.3.1\r\nwrapt                              1.11.2\r\nwurlitzer                          2.0.0\r\nxarray                             0.15.1\r\nxgboost                            1.1.0\r\nxlrd                               1.2.0\r\nXlsxWriter                         1.2.8\r\nxlwt                               1.3.0\r\nxvfbwrapper                        0.2.9\r\nyapf                               0.29.0\r\nyarl                               1.4.2\r\nyellowbrick                        1.1\r\nzict                               2.0.0\r\nzipp                               3.1.0\r\n","name":"stdout"},{"output_type":"stream","text":"Note: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport re\nimport json\nfrom transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, AlbertTokenizer, AlbertModel, AlbertPreTrainedModel, AlbertConfig\nimport time\nfrom apex import amp\n\npath='/kaggle/input/models/models'\nos.chdir(path)\nos.listdir(path)\n\ndef reduce1(n_candidate=10, th_candidate=0.2):\n\n    class TFQADataset(Dataset):\n        def __init__(self, id_list):\n            self.id_list=id_list \n        def __len__(self):\n            return len(self.id_list)\n        def __getitem__(self, index):\n            return self.id_list[index]\n\n    class Collator(object):\n        def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len=384, max_question_len=64):\n            self.data_dict = data_dict\n            self.new_token_dict = new_token_dict\n            self.tokenizer = tokenizer\n            self.max_seq_len = max_seq_len\n            self.max_question_len = max_question_len\n\n        def _get_input_ids(self, doc_id, candidate_index):\n            data = self.data_dict[doc_id]\n            question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_question_len]\n            doc_words = data['document_text'].split()\n\n            max_answer_tokens = self.max_seq_len-len(question_tokens)-3 # [CLS],[SEP],[SEP]\n            candidate = data['long_answer_candidates'][candidate_index]\n            candidate_start = candidate['start_token']\n            candidate_end = candidate['end_token']\n            candidate_words = doc_words[candidate_start:candidate_end]  \n            # Loop through to add html tokens as new tokens here\n            for i, word in enumerate(candidate_words):\n                if re.match(r'<.+>', word):\n                    if word in self.new_token_dict: \n                        candidate_words[i] = self.new_token_dict[word]\n                    else:\n                        candidate_words[i] = '<'     \n\n            words_to_tokens_index = []\n            candidate_tokens = []\n            for i, word in enumerate(candidate_words):\n                words_to_tokens_index.append(len(candidate_tokens))\n                tokens = self.tokenizer.tokenize(word)\n                if len(candidate_tokens)+len(tokens) > max_answer_tokens: # token length cannot be longer than the global max length (360)\n                    break\n                for token in tokens:\n                    candidate_tokens.append(token)\n\n            input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + candidate_tokens + ['[SEP]']\n            input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n\n            return input_ids, len(input_ids)\n        \n        def __call__(self, batch_ids):\n            batch_size = len(batch_ids)\n\n            # pre-compute all the input within the batch without padding to determine the actual batch max sequence length\n            batch_input_ids_temp = []\n            batch_seq_len = []\n\n            for i, (doc_id, candidate_index) in enumerate(batch_ids):\n                input_ids, seq_len = self._get_input_ids(doc_id, candidate_index)\n                batch_input_ids_temp.append(input_ids)\n                batch_seq_len.append(seq_len)\n\n            batch_max_seq_len = max(batch_seq_len) # set max sequence length to be the maximun length in a batch, to save computation \n            batch_input_ids = np.zeros((batch_size, batch_max_seq_len), dtype=np.int64)\n            batch_token_type_ids = np.ones((batch_size, batch_max_seq_len), dtype=np.int64)\n\n            for i in range(batch_size):\n                input_ids = batch_input_ids_temp[i]\n                batch_input_ids[i, :len(input_ids)] = input_ids\n                batch_token_type_ids[i, :len(input_ids)] = [0 if k<=input_ids.index(102) else 1 for k in range(len(input_ids))]\n\n            batch_attention_mask = batch_input_ids > 0\n\n            return torch.from_numpy(batch_input_ids),torch.from_numpy(batch_attention_mask),torch.from_numpy(batch_token_type_ids)\n\n    # https://www.kaggle.com/sakami/tfqa-pytorch-baseline\n    class BertForQuestionAnswering(BertPreTrainedModel):\n\n        def __init__(self, config):\n            super(BertForQuestionAnswering, self).__init__(config)\n            self.bert = BertModel(config)\n            self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n            self.init_weights()\n\n        def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n            outputs = self.bert(input_ids,\n                                attention_mask=attention_mask,\n                                token_type_ids=token_type_ids,\n                                position_ids=position_ids, \n                                head_mask=head_mask)\n\n            sequence_output = outputs[0]\n            pooled_output = outputs[1]\n\n            # predict start & end position\n            qa_logits = self.qa_outputs(sequence_output)\n            start_logits, end_logits = qa_logits.split(1, dim=-1)\n            start_logits = start_logits.squeeze(-1)\n            end_logits = end_logits.squeeze(-1)\n    \n            # classification\n            pooled_output = self.dropout(pooled_output)\n            classifier_logits = self.classifier(pooled_output)\n\n            return start_logits, end_logits, classifier_logits\n\n\n    # prepare input\n    json_dir = '../../../input/tensorflow2-question-answering/simplified-nq-test.jsonl'\n\n    # The id_candidate_list keeps all the combination of document ids and their candidates. So we essentially run predictions on all the candidates.\n    id_candidate_list = []\n    # Store the lengths of each candidate for rearranging based on candidate length, this can help improve inference speed significantly.\n    id_candidate_len_list = [] \n    # Keep a dictionary for length checking.\n    id_candidate_len_dict = {}\n    # list of document ids.\n    id_list = []\n    # Keeps the texts and candidates.\n    data_dict = {}\n    # for debugging only\n    max_data = 9999999999\n    with open(json_dir) as f:\n        for n, line in tqdm(enumerate(f)):\n            if n > max_data:\n                break\n            data = json.loads(line)\n            data_id = data['example_id']\n            id_list.append(data_id)\n\n            # initialize data_dict\n            data_dict[data_id] = {\n                                  'document_text': data['document_text'],\n                                  'question_text': data['question_text'], \n                                  'long_answer_candidates': data['long_answer_candidates'],                \n                                 }\n        \n            question_len = len(data['question_text'].split())\n\n            # We use the wite space tokenzied version to estimate candidate length here.\n            for i in range(len(data['long_answer_candidates'])):\n                id_candidate_list.append((data_id, i))\n                candidate_len = question_len+data['long_answer_candidates'][i]['end_token']-data['long_answer_candidates'][i]['start_token']\n                id_candidate_len_list.append(candidate_len)\n                id_candidate_len_dict[(data_id, i)] = candidate_len\n\n    print(len(id_candidate_list))\n\n    # Sort based on the length of each candidate.\n    id_candidate_len_list = np.array(id_candidate_len_list)\n    sorted_index = np.argsort(id_candidate_len_list)\n    id_candidate_list_sorted = []\n    for i in range(len(id_candidate_list)):\n        id_candidate_list_sorted.append(id_candidate_list[sorted_index[i]])\n\n\n    # hyperparameters\n    max_seq_len = 360\n    max_question_len = 64\n    batch_size = int(768/4)\n\n    # build model\n    model_path = '../models/models/bert/'\n    config = BertConfig.from_pretrained(model_path)\n    config.num_labels = 5\n    config.vocab_size = 30531\n    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n    model = BertForQuestionAnswering.from_pretrained('../models/models/bert/', config=config)\n\n    # add new tokens\n    new_token_dict = {\n                      '<P>':'qw1',\n                      '<Table>':'qw2',\n                      '<Tr>':'qw3',\n                      '<Ul>':'qw4',\n                      '<Ol>':'qw5',\n                      '<Fl>':'qw6',\n                      '<Li>':'qw7',\n                      '<Dd>':'qw8',\n                      '<Dt>':'qw9',\n                     }\n    new_token_list = [\n                      'qw1',\n                      'qw2',\n                      'qw3',\n                      'qw4',\n                      'qw5',\n                      'qw6',\n                      'qw7',\n                      'qw8',\n                      'qw9',\n                     ]\n\n    num_added_toks = tokenizer.add_tokens(new_token_list)\n    print('We have added', num_added_toks, 'tokens')\n    model.resize_token_embeddings(len(tokenizer))\n\n\n    model.cuda()\n    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n\n    # testing\n\n    # iterator for testing\n    test_datagen = TFQADataset(id_list=id_candidate_list_sorted)\n    test_collate = Collator(data_dict=data_dict, \n                            new_token_dict=new_token_dict,\n                            tokenizer=tokenizer, \n                            max_seq_len=max_seq_len, \n                            max_question_len=max_question_len)\n    test_generator = DataLoader(dataset=test_datagen,\n                                collate_fn=test_collate,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                num_workers=16,\n                                pin_memory=True)\n\n\n    model.eval()\n    test_prob3 = np.zeros((len(id_candidate_list_sorted),5),dtype=np.float32) # class\n    for j,(batch_input_ids, batch_attention_mask, batch_token_type_ids) in tqdm(enumerate(test_generator)):\n        with torch.no_grad():\n            start = j*batch_size\n            end = start+batch_size\n            if j == len(test_generator)-1:\n                end = len(test_generator.dataset)\n            batch_input_ids = batch_input_ids.cuda()\n            batch_attention_mask = batch_attention_mask.cuda()\n            batch_token_type_ids = batch_token_type_ids.cuda()\n            # We don't need the span output here.\n            _, _, logits3 = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n            test_prob3[start:end] += F.softmax(logits3,dim=1).cpu().data.numpy()\n\n\n    # initialize a temp dictionary\n    temp_dict = {}\n    for doc_id in id_list:\n        temp_dict[doc_id] = np.zeros((len(data_dict[doc_id]['long_answer_candidates']),),dtype=np.float32)\n\n    # input long answer probs into the temp dictionary\n    for i, (doc_id, candidate_index) in tqdm(enumerate(id_candidate_list_sorted)):\n        temp_dict[doc_id][candidate_index] = 1.0 - test_prob3[i,0] # 1-no_answer_score\n\n    # get list of survived id-candidates\n    id_candidate_list1 = []\n    id_candidate_len_list1 = []\n    for doc_id in tqdm(id_list):\n        long_prob_array = temp_dict[doc_id].copy()\n        sorted_index = np.argsort(long_prob_array)[::-1]\n        count = 0\n        for n in range(len(sorted_index)):\n            if count>=n_candidate:\n                break\n            else:\n                if temp_dict[doc_id][sorted_index[n]]>th_candidate:\n                    id_candidate_list1.append((doc_id, sorted_index[n]))\n                    id_candidate_len_list1.append(id_candidate_len_dict[(doc_id, sorted_index[n])])\n                    count += 1\n\n    # sort and return\n    sorted_index = np.argsort(id_candidate_len_list1)\n    id_candidate_list_sorted1 = []\n    for i in range(len(id_candidate_list1)):\n        id_candidate_list_sorted1.append(id_candidate_list1[sorted_index[i]])\n\n    print(len(id_candidate_list_sorted1))\n\n    return data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted1\n\n\ndef reduce2(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, n_candidate=10, th_candidate=0.2):\n\n    class TFQADataset(Dataset):\n        def __init__(self, id_list):\n            self.id_list=id_list \n        def __len__(self):\n            return len(self.id_list)\n        def __getitem__(self, index):\n            return self.id_list[index]\n\n    class Collator(object):\n        def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len=384, max_question_len=64):\n            self.data_dict = data_dict\n            self.new_token_dict = new_token_dict\n            self.tokenizer = tokenizer\n            self.max_seq_len = max_seq_len\n            self.max_question_len = max_question_len\n\n        def _get_input_ids(self, doc_id, candidate_index):\n            data = self.data_dict[doc_id]\n            question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_question_len]\n            doc_words = data['document_text'].split()\n\n            max_answer_tokens = self.max_seq_len-len(question_tokens)-3 # [CLS],[SEP],[SEP]\n            candidate = data['long_answer_candidates'][candidate_index]\n            candidate_start = candidate['start_token']\n            candidate_end = candidate['end_token']\n            candidate_words = doc_words[candidate_start:candidate_end]  \n            for i, word in enumerate(candidate_words):\n                if re.match(r'<.+>', word):\n                    if word in self.new_token_dict: \n                        candidate_words[i] = self.new_token_dict[word]\n                    else:\n                        candidate_words[i] = '<'     \n\n            words_to_tokens_index = []\n            candidate_tokens = []\n            for i, word in enumerate(candidate_words):\n                words_to_tokens_index.append(len(candidate_tokens))\n                tokens = self.tokenizer.tokenize(word)\n                if len(candidate_tokens)+len(tokens) > max_answer_tokens:\n                    break\n                for token in tokens:\n                    candidate_tokens.append(token)\n\n            input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + candidate_tokens + ['[SEP]']\n            input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n\n            return input_ids, len(input_ids)\n        \n        def __call__(self, batch_ids):\n            batch_size = len(batch_ids)\n\n            batch_input_ids_temp = []\n            batch_seq_len = []\n\n            for i, (doc_id, candidate_index) in enumerate(batch_ids):\n                input_ids, seq_len = self._get_input_ids(doc_id, candidate_index)\n                batch_input_ids_temp.append(input_ids)\n                batch_seq_len.append(seq_len)\n\n            batch_max_seq_len = max(batch_seq_len)\n            batch_input_ids = np.zeros((batch_size, batch_max_seq_len), dtype=np.int64)\n            batch_token_type_ids = np.ones((batch_size, batch_max_seq_len), dtype=np.int64)\n\n            for i in range(batch_size):\n                input_ids = batch_input_ids_temp[i]\n                batch_input_ids[i, :len(input_ids)] = input_ids\n                batch_token_type_ids[i, :len(input_ids)] = [0 if k<=input_ids.index(102) else 1 for k in range(len(input_ids))]\n\n            batch_attention_mask = batch_input_ids > 0\n\n            return torch.from_numpy(batch_input_ids),torch.from_numpy(batch_attention_mask),torch.from_numpy(batch_token_type_ids)\n\n\n    class BertForQuestionAnswering(BertPreTrainedModel):\n\n        def __init__(self, config):\n            super(BertForQuestionAnswering, self).__init__(config)\n            self.bert = BertModel(config)\n            self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n            self.init_weights()\n\n        def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n            outputs = self.bert(input_ids,\n                                attention_mask=attention_mask,\n                                token_type_ids=token_type_ids,\n                                position_ids=position_ids, \n                                head_mask=head_mask)\n\n            sequence_output = outputs[0]\n            pooled_output = outputs[1]\n\n            # predict start & end position\n            qa_logits = self.qa_outputs(sequence_output)\n            start_logits, end_logits = qa_logits.split(1, dim=-1)\n            start_logits = start_logits.squeeze(-1)\n            end_logits = end_logits.squeeze(-1)\n    \n            # classification\n            pooled_output = self.dropout(pooled_output)\n            classifier_logits = self.classifier(pooled_output)\n\n            return start_logits, end_logits, classifier_logits\n\n\n    # hyperparameters\n    max_seq_len = 360\n    max_question_len = 64\n    batch_size = int(384/4)\n\n\n    # build model\n    model_path = '../models/models/bert/'\n    config = BertConfig.from_pretrained(model_path)\n    config.num_labels = 5\n    config.vocab_size = 30531\n    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n    model = BertForQuestionAnswering.from_pretrained('../models/models/bert/', config=config)\n\n    # add new tokens\n    new_token_dict = {\n                      '<P>':'qw1',\n                      '<Table>':'qw2',\n                      '<Tr>':'qw3',\n                      '<Ul>':'qw4',\n                      '<Ol>':'qw5',\n                      '<Fl>':'qw6',\n                      '<Li>':'qw7',\n                      '<Dd>':'qw8',\n                      '<Dt>':'qw9',\n                     }\n    new_token_list = [\n                      'qw1',\n                      'qw2',\n                      'qw3',\n                      'qw4',\n                      'qw5',\n                      'qw6',\n                      'qw7',\n                      'qw8',\n                      'qw9',\n                     ]\n\n    num_added_toks = tokenizer.add_tokens(new_token_list)\n    print('We have added', num_added_toks, 'tokens')\n    model.resize_token_embeddings(len(tokenizer))\n\n\n    model.cuda()\n    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n\n    # testing\n\n    # iterator for testing\n    test_datagen = TFQADataset(id_list=id_candidate_list_sorted)\n    test_collate = Collator(data_dict=data_dict, \n                            new_token_dict=new_token_dict,\n                            tokenizer=tokenizer, \n                            max_seq_len=max_seq_len, \n                            max_question_len=max_question_len)\n    test_generator = DataLoader(dataset=test_datagen,\n                                collate_fn=test_collate,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                num_workers=16,\n                                pin_memory=True)\n\n\n    model.eval()\n    test_prob3 = np.zeros((len(id_candidate_list_sorted),5),dtype=np.float32) # class\n    for j,(batch_input_ids, batch_attention_mask, batch_token_type_ids) in tqdm(enumerate(test_generator)):\n        with torch.no_grad():\n            start = j*batch_size\n            end = start+batch_size\n            if j == len(test_generator)-1:\n                end = len(test_generator.dataset)\n            batch_input_ids = batch_input_ids.cuda()\n            batch_attention_mask = batch_attention_mask.cuda()\n            batch_token_type_ids = batch_token_type_ids.cuda()\n            _, _, logits3 = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n            test_prob3[start:end] += F.softmax(logits3,dim=1).cpu().data.numpy()\n\n\n    # initialize a temp dictionary\n    temp_dict = {}\n    for doc_id in id_list:\n        temp_dict[doc_id] = np.zeros((len(data_dict[doc_id]['long_answer_candidates']),),dtype=np.float32)\n\n    # input long answer probs into the temp dictionary\n    for i, (doc_id, candidate_index) in tqdm(enumerate(id_candidate_list_sorted)):\n        temp_dict[doc_id][candidate_index] = 1.0 - test_prob3[i,0] # 1-no_answer_score\n\n    # get list of survived id-candidates\n    id_candidate_list1 = []\n    id_candidate_len_list1 = []\n    for doc_id in tqdm(id_list):\n        long_prob_array = temp_dict[doc_id].copy()\n        sorted_index = np.argsort(long_prob_array)[::-1]\n        count = 0\n        for n in range(len(sorted_index)):\n            if count>=n_candidate:\n                break\n            else:\n                if temp_dict[doc_id][sorted_index[n]]>th_candidate:\n                    id_candidate_list1.append((doc_id, sorted_index[n]))\n                    id_candidate_len_list1.append(id_candidate_len_dict[(doc_id, sorted_index[n])])\n                    count += 1\n\n    # sort and return\n    sorted_index = np.argsort(id_candidate_len_list1)\n    id_candidate_list_sorted1 = []\n    for i in range(len(id_candidate_list1)):\n        id_candidate_list_sorted1.append(id_candidate_list1[sorted_index[i]])\n\n    print(len(id_candidate_list_sorted1))\n\n    return id_candidate_list_sorted1\n\n\ndef albert_predict(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, model_dir, word_len):\n\n    class TFQADataset(Dataset):\n        def __init__(self, id_list):\n            self.id_list=id_list \n        def __len__(self):\n            return len(self.id_list)\n        def __getitem__(self, index):\n            return self.id_list[index]\n\n    class Collator(object):\n        def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len=384, max_question_len=64):\n            self.data_dict = data_dict\n            self.new_token_dict = new_token_dict\n            self.tokenizer = tokenizer\n            self.max_seq_len = max_seq_len\n            self.max_question_len = max_question_len\n\n        def _get_input_ids(self, doc_id, candidate_index):\n            data = self.data_dict[doc_id]\n            question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_question_len]\n            doc_words = data['document_text'].split()\n\n            max_answer_tokens = self.max_seq_len-len(question_tokens)-3 # [CLS],[SEP],[SEP]\n            candidate = data['long_answer_candidates'][candidate_index]\n            candidate_start = candidate['start_token']\n            candidate_end = candidate['end_token']\n            candidate_words = doc_words[candidate_start:candidate_end]  \n            for i, word in enumerate(candidate_words):\n                if re.match(r'<.+>', word):\n                    if word in self.new_token_dict: \n                        candidate_words[i] = self.new_token_dict[word]\n                    else:\n                        candidate_words[i] = 'qw99'    \n\n            words_to_tokens_index = []\n            tokens_to_words_index = []\n            candidate_tokens = []\n            for i, word in enumerate(candidate_words):\n                words_to_tokens_index.append(len(candidate_tokens))\n                tokens = self.tokenizer.tokenize(word)\n                if len(candidate_tokens)+len(tokens) > max_answer_tokens:\n                    break\n                for token in tokens:\n                    tokens_to_words_index.append(i)\n                    candidate_tokens.append(token)\n\n            input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + candidate_tokens + ['[SEP]']\n            input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n\n            return input_ids, words_to_tokens_index, len(input_ids), len(question_tokens)+2\n        \n        def __call__(self, batch_ids):\n            batch_size = len(batch_ids)\n\n            batch_input_ids_temp = []\n            batch_seq_len = []\n\n            batch_offset = []\n            batch_words_to_tokens_index = []\n\n            for i, (doc_id, candidate_index) in enumerate(batch_ids):\n                input_ids, words_to_tokens_index, seq_len, offset = self._get_input_ids(doc_id, candidate_index)\n                batch_input_ids_temp.append(input_ids)\n                batch_seq_len.append(seq_len)\n                batch_offset.append(offset)\n                batch_words_to_tokens_index.append(words_to_tokens_index)\n\n            batch_max_seq_len = max(batch_seq_len)\n            batch_input_ids = np.zeros((batch_size, batch_max_seq_len), dtype=np.int64)\n            batch_token_type_ids = np.ones((batch_size, batch_max_seq_len), dtype=np.int64)\n\n            for i in range(batch_size):\n                input_ids = batch_input_ids_temp[i]\n                batch_input_ids[i, :len(input_ids)] = input_ids\n                batch_token_type_ids[i, :len(input_ids)] = [0 if k<=input_ids.index(3) else 1 for k in range(len(input_ids))]\n\n            batch_attention_mask = batch_input_ids > 0\n\n            return torch.from_numpy(batch_input_ids), torch.from_numpy(batch_attention_mask), torch.from_numpy(batch_token_type_ids), batch_words_to_tokens_index, batch_offset, batch_max_seq_len\n\n\n    class AlbertForQuestionAnswering(AlbertPreTrainedModel):\n\n        def __init__(self, config):\n            super(AlbertForQuestionAnswering, self).__init__(config)\n            self.albert = AlbertModel(config)\n            self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n            self.init_weights()\n\n        def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n            outputs = self.albert(input_ids,\n                                  attention_mask=attention_mask,\n                                  token_type_ids=token_type_ids,\n                                  position_ids=position_ids, \n                                  head_mask=head_mask)\n\n            sequence_output = outputs[0]\n            pooled_output = outputs[1]\n\n            # predict start & end position\n            qa_logits = self.qa_outputs(sequence_output)\n            start_logits, end_logits = qa_logits.split(1, dim=-1)\n            start_logits = start_logits.squeeze(-1)\n            end_logits = end_logits.squeeze(-1)\n    \n            # classification\n            pooled_output = self.dropout(pooled_output)\n            classifier_logits = self.classifier(pooled_output)\n\n            return start_logits, end_logits, classifier_logits\n\n\n    # hyperparameters\n    max_seq_len = 360\n    max_question_len = 64\n    batch_size = int(128/4)\n\n\n    # build model\n    model_path = '../models/models/albert/'\n    config = AlbertConfig.from_pretrained(model_path)\n    config.num_labels = 5\n    config.vocab_size = 30010\n    tokenizer = AlbertTokenizer.from_pretrained(model_path, do_lower_case=True)\n    model = AlbertForQuestionAnswering.from_pretrained(model_dir, config=config)\n\n    # add new tokens\n    new_token_dict = {\n                      '<P>':'qw1',\n                      '<Table>':'qw2',\n                      '<Tr>':'qw3',\n                      '<Ul>':'qw4',\n                      '<Ol>':'qw5',\n                      '<Fl>':'qw6',\n                      '<Li>':'qw7',\n                      '<Dd>':'qw8',\n                      '<Dt>':'qw9',\n                     }\n    new_token_list = [\n                      'qw1',\n                      'qw2',\n                      'qw3',\n                      'qw4',\n                      'qw5',\n                      'qw6',\n                      'qw7',\n                      'qw8',\n                      'qw9',\n                      'qw99',\n                     ]\n\n    num_added_toks = tokenizer.add_tokens(new_token_list)\n    print('We have added', num_added_toks, 'tokens')\n    model.resize_token_embeddings(len(tokenizer))\n\n\n    model.cuda()\n    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n\n    # testing\n\n    # iterator for testing\n    test_datagen = TFQADataset(id_list=id_candidate_list_sorted)\n    test_collate = Collator(data_dict=data_dict, \n                            new_token_dict=new_token_dict,\n                            tokenizer=tokenizer, \n                            max_seq_len=max_seq_len, \n                            max_question_len=max_question_len)\n    test_generator = DataLoader(dataset=test_datagen,\n                                collate_fn=test_collate,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                num_workers=16,\n                                pin_memory=True)\n\n\n    model.eval()\n    list_offset = []\n    list_words_to_tokens_index = []\n    test_prob1 = np.zeros((len(id_candidate_list_sorted),max_seq_len),dtype=np.float32) # start\n    test_prob2 = np.zeros((len(id_candidate_list_sorted),max_seq_len),dtype=np.float32) # end\n    test_prob3 = np.zeros((len(id_candidate_list_sorted),5),dtype=np.float32) # class\n    for j,(batch_input_ids, batch_attention_mask, batch_token_type_ids, batch_words_to_tokens_index, batch_offset, batch_max_seq_len) in tqdm(enumerate(test_generator)):\n        with torch.no_grad():\n            start = j*batch_size\n            end = start+batch_size\n            if j == len(test_generator)-1:\n                end = len(test_generator.dataset)\n            batch_input_ids = batch_input_ids.cuda()\n            batch_attention_mask = batch_attention_mask.cuda()\n            batch_token_type_ids = batch_token_type_ids.cuda()\n            logits1, logits2, logits3 = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n            test_prob1[start:end, :batch_max_seq_len] += F.softmax(logits1,dim=1).cpu().data.numpy()\n            test_prob2[start:end, :batch_max_seq_len] += F.softmax(logits2,dim=1).cpu().data.numpy()\n            test_prob3[start:end] += F.softmax(logits3,dim=1).cpu().data.numpy()\n            list_words_to_tokens_index += batch_words_to_tokens_index\n            list_offset += batch_offset\n\n    test_word_prob1 = np.zeros((len(id_candidate_list_sorted),word_len),dtype=np.float32) # start\n    test_word_prob2 = np.zeros((len(id_candidate_list_sorted),word_len),dtype=np.float32) # end\n    for i in range(len(id_candidate_list_sorted)):\n        for j in range(len(list_words_to_tokens_index[i])):\n            test_word_prob1[i,j] = test_prob1[i, list_words_to_tokens_index[i][j]+list_offset[i]]\n            test_word_prob2[i,j] = test_prob2[i, list_words_to_tokens_index[i][j]+list_offset[i]]\n\n\n    return test_word_prob1, test_word_prob2, test_prob3\n\n\n# This function performs a full prediction on the validation set using a fast model (bert-base) to reduce the candidates for larger model predictions.\n# Propose only the top-k (top10 in this case) most probable candidates from each document, each candidate must have long answer probability larger than a threshold (0.2), the rest candidates are set to negative.\n# id_candidate_list_sorted stores (document id, candidate number) as keys, each of each contains its long answer probability (score).\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\ndata_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted = reduce1(n_candidate=10, th_candidate=0.2)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n# Futher reduce the number of candidates from top10 to top4.\nstart_time = time.time()\nid_candidate_list_sorted = reduce2(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, n_candidate=4, th_candidate=0.35)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n# Acutual predictions start here. \nstart_time = time.time()\n# We keep word-level posterior start and end vectors. Since the token-level length is set to 360, this number should be enough for word-level.\nword_len = 360\n# Initialize start and end \"\"word-level\"\" prob vectors for easier probability averaging between different models equiped with different tokenizers.\nstart_prob = np.zeros((len(id_candidate_list_sorted),word_len),dtype=np.float32)\nend_prob = np.zeros((len(id_candidate_list_sorted),word_len),dtype=np.float32)\nstart_label = np.zeros((len(id_candidate_list_sorted),),dtype=int)\nend_label = np.zeros((len(id_candidate_list_sorted),),dtype=int)\n# class_prob stores the 5-class classifier prob outputs.\n# no answer(0), long but not short answer(1), short answer with span(2), NO(3), YES(4)\nclass_prob = np.zeros((len(id_candidate_list_sorted),5),dtype=np.float32)\n\n# Perform prediction using two albert-xxl and two bert-large models. Weighted average of both long and short predictions for ensembling.\n# model_dir = '../albert-xxlarge-v2_2/weights/epoch2/'\n# test_prob1, test_prob2, test_prob3 = albert_predict(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, model_dir, word_len)\n# start_prob += 0.3*test_prob1\n# end_prob += 0.3*test_prob2\n# class_prob += 0.3*test_prob3\n# model_dir = '../albert-xxlarge-v2_3/weights/epoch2/'\n# test_prob1, test_prob2, test_prob3 = albert_predict(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, model_dir, word_len)\n# start_prob += 0.3*test_prob1\n# end_prob += 0.3*test_prob2\n# class_prob += 0.3*test_prob3\n# model_dir = '../bert-large-uncased_4/weights/epoch3/'\n# test_prob1, test_prob2, test_prob3 = bert_large_predict(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, model_dir, word_len)\n# start_prob += 0.2*test_prob1\n# end_prob += 0.2*test_prob2\n# class_prob += 0.2*test_prob3\n\nprint(\"before albert predicting\")\n\nmodel_dir = '../models/models/albert/'\ntest_prob1, test_prob2, test_prob3 = albert_predict(data_dict, id_list, id_candidate_len_dict, id_candidate_list_sorted, model_dir, word_len)\n\nstart_prob += test_prob1\nend_prob += test_prob2\nclass_prob += test_prob3\n\nprint(\"before saving\")\n\n# The start and end words have the largest probabilities.\nstart_label = np.argmax(start_prob, axis=1)\nend_label = np.argmax(end_prob, axis=1)\n\n\n\n# initialize a temporary dictionary to store prediction values.\ntemp_dict = {}\nfor doc_id in id_list:\n    temp_dict[doc_id] = {\n                         'long_answer': {'start_token': -1, 'end_token': -1},\n                         'long_answer_score': -1.0,\n                         'short_answers': [{'start_token': -1, 'end_token': -1}],\n                         'short_answers_score': -1.0,\n                         'yes_no_answer': 'NONE'\n                        }\n\n# from cadidates to document\nfor i, (doc_id, candidate_index) in tqdm(enumerate(id_candidate_list_sorted)):\n    # process long answer\n    long_answer_score = 1.0 - class_prob[i,0] # 1 - no_answer_score\n    if long_answer_score > temp_dict[doc_id]['long_answer_score']:\n        temp_dict[doc_id]['long_answer_score'] = long_answer_score\n        temp_dict[doc_id]['long_answer']['start_token'] = data_dict[doc_id]['long_answer_candidates'][candidate_index]['start_token']\n        temp_dict[doc_id]['long_answer']['end_token'] = data_dict[doc_id]['long_answer_candidates'][candidate_index]['end_token']\n        # process short answer\n        short_answer_score = 1.0 - class_prob[i,0] - class_prob[i,1] # 1 - no_answer_score - long_but_not_short_answer_score\n        temp_dict[doc_id]['short_answers_score'] = short_answer_score\n\n        temp_dict[doc_id]['short_answers'][0]['start_token'] = -1\n        temp_dict[doc_id]['short_answers'][0]['end_token'] = -1\n        temp_dict[doc_id]['yes_no_answer'] = 'NONE'\n        if max([class_prob[i,3], class_prob[i,4]]) > class_prob[i,2]:\n            if class_prob[i,3] > class_prob[i,4]:\n                temp_dict[doc_id]['yes_no_answer'] = 'NO'\n            else:\n                temp_dict[doc_id]['yes_no_answer'] = 'YES'\n        else:\n            short_start_word = int(start_label[i]) + data_dict[doc_id]['long_answer_candidates'][candidate_index]['start_token']\n            short_end_word = int(end_label[i]) + data_dict[doc_id]['long_answer_candidates'][candidate_index]['start_token']\n            if short_end_word > short_start_word:\n                temp_dict[doc_id]['short_answers'][0]['start_token'] = short_start_word\n                temp_dict[doc_id]['short_answers'][0]['end_token'] = short_end_word\n\n# Copy the temporary dictionary into the final dictionary that meets the required format for validation.\nfinal_dict = {}\nfinal_dict['predictions'] = []\nfor doc_id in id_list:\n    prediction_dict = {\n                       'example_id': doc_id,\n                       'long_answer': {'start_byte': -1, 'end_byte': -1, 'start_token': temp_dict[doc_id]['long_answer']['start_token'], 'end_token': temp_dict[doc_id]['long_answer']['end_token']},\n                       'long_answer_score': temp_dict[doc_id]['long_answer_score'],\n                       'short_answers': [{'start_byte': -1, 'end_byte': -1, 'start_token': temp_dict[doc_id]['short_answers'][0]['start_token'], 'end_token': temp_dict[doc_id]['short_answers'][0]['end_token']}],\n                       'short_answers_score': temp_dict[doc_id]['short_answers_score'],\n                       'yes_no_answer': temp_dict[doc_id]['yes_no_answer']\n                      }\n    final_dict['predictions'].append(prediction_dict)\n\n# with open('predictions.json', 'w') as fp:\n#     json.dump(final_dict, fp)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":15,"outputs":[{"output_type":"stream","text":"346it [00:00, 1435.90it/s]\n","name":"stderr"},{"output_type":"stream","text":"45163\nWe have added 9 tokens\n","name":"stdout"},{"output_type":"stream","text":"236it [03:42,  1.06it/s]\n45163it [00:00, 180241.87it/s]\n100%|██████████| 346/346 [00:00<00:00, 2792.37it/s]\n","name":"stderr"},{"output_type":"stream","text":"1427\n--- 227.18968629837036 seconds ---\nWe have added 9 tokens\n","name":"stdout"},{"output_type":"stream","text":"15it [00:19,  1.28s/it]\n1427it [00:00, 158710.01it/s]\n100%|██████████| 346/346 [00:00<00:00, 4147.36it/s]","name":"stderr"},{"output_type":"stream","text":"714\n--- 23.366134881973267 seconds ---\nbefore albert predicting\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"},{"output_type":"stream","text":"We have added 10 tokens\n","name":"stdout"},{"output_type":"stream","text":"23it [00:56,  2.47s/it]\n","name":"stderr"},{"output_type":"stream","text":"before saving\n","name":"stdout"},{"output_type":"stream","text":"714it [00:00, 18911.64it/s]\n","name":"stderr"},{"output_type":"error","ename":"OSError","evalue":"[Errno 30] Read-only file system: 'predictions.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-4b3e65027abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# print(\"after final saving\", flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m# dump to json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'predictions.json'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport sys\nimport collections\nimport json\n\nanswers_df = pd.DataFrame.from_dict(final_dict)\n# answers_df = pd.read_json(\"../input/predictions/predictions.json\")\n# answers_df.head()\n# answers_df[\"predictions\"][1]\n\n\ndef df_long_index_score(df):\n    answers = []\n    cont = 0\n    for i in range(len(df)):\n        index = {}\n        if df[\"predictions\"][i]['long_answer_score'] > 0:\n            index['start'] = df[\"predictions\"][i]['long_answer']['start_token']\n            index['end'] = df[\"predictions\"][i]['long_answer']['end_token']\n            index['score'] = df[\"predictions\"][i]['long_answer_score']\n            index = [index]\n            answers.append(index)\n        else:\n            answers.append([])\n    return answers\n\n\ndef df_short_index_score(df):\n    answers = []\n    cont = 0\n    for i in range(len(df)):\n        index = {}\n        if df[\"predictions\"][i]['short_answers_score'] > 0 and df[\"predictions\"][i]['short_answers'][0]['start_token'] != -1:\n            index['start'] = df[\"predictions\"][i]['short_answers'][0]['start_token']\n            index['end'] = df[\"predictions\"][i]['short_answers'][0]['end_token']\n            index['score'] = df[\"predictions\"][i]['short_answers_score']\n            index = [index]\n            answers.append(index)\n        else:\n            answers.append([])\n    return answers\n\n\ndef df_example_id(df):\n    return df['example_id']\n\ndef create_answer(entry):\n    answer = []\n    for e in entry:\n        answer.append(str(e['start']) + ':' + str(e['end']))\n    if not answer:\n        answer = \"\"\n    return \", \".join(answer)\n\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# answers_df['answer'] = answers_df['predictions'].apply(df_long_index_score)\nanswers_df['long_indexes_and_scores'] = df_long_index_score(answers_df)\nanswers_df['short_indexes_and_scores'] = df_short_index_score(answers_df)\nanswers_df['example_id'] = answers_df['predictions'].apply(df_example_id)\n\n\n# list(answers_df[answers_df['example_id'].isin([\"-332839753184669166\"])][\"predictions\"])[0]['example_id']\n# answers_df[\"predictions\"][0]['short_answers'][0]['start_token']\n\n# answers_df[answers_df['example_id'].isin([\"-332839753184669166\"])] # [\"predictions\"]\n\nanswers_df = answers_df.drop(['predictions'], axis=1)\n\n# answers_df.head()\n\nanswers_df[\"long_answer\"] = answers_df['long_indexes_and_scores'].apply(\n    create_answer)\nanswers_df[\"short_answer\"] = answers_df['short_indexes_and_scores'].apply(\n    create_answer)\nanswers_df[\"example_id\"] = answers_df['example_id'].apply(lambda q: str(q))\n\nlong_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"long_answer\"]))\nshort_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"short_answer\"]))\n\n# answers_df.head()\n\nanswers_df = answers_df.drop(\n    ['long_indexes_and_scores', 'short_indexes_and_scores'], axis=1)\n# answers_df.head()\n\nsample_submission = pd.read_csv(\n    \"../../../input/tensorflow2-question-answering/sample_submission.csv\")\n\nlong_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\n    \"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\nshort_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\n    \"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\n    \"_long\"), \"PredictionString\"] = long_prediction_strings\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\n    \"_short\"), \"PredictionString\"] = short_prediction_strings\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"                     example_id PredictionString\n0     -1011141123527297803_long                 \n1    -1011141123527297803_short                 \n2     -1028916936938579349_long                 \n3    -1028916936938579349_short                 \n4     -1055197305756217938_long                 \n..                          ...              ...\n687    930196817123445627_short                 \n688     934950704129184964_long        3878:4153\n689    934950704129184964_short        4127:4128\n690     958723574737344087_long         938:2597\n691    958723574737344087_short        1000:1001\n\n[692 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>example_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1011141123527297803_long</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1011141123527297803_short</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1028916936938579349_long</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1028916936938579349_short</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1055197305756217938_long</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>687</th>\n      <td>930196817123445627_short</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>688</th>\n      <td>934950704129184964_long</td>\n      <td>3878:4153</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>934950704129184964_short</td>\n      <td>4127:4128</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>958723574737344087_long</td>\n      <td>938:2597</td>\n    </tr>\n    <tr>\n      <th>691</th>\n      <td>958723574737344087_short</td>\n      <td>1000:1001</td>\n    </tr>\n  </tbody>\n</table>\n<p>692 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path='../../..'\nos.chdir(path)\n\nsample_submission.to_csv('submission.csv', index=False)","execution_count":31,"outputs":[{"output_type":"error","ename":"OSError","evalue":"[Errno 30] Read-only file system: 'submission.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-16642b31bb08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'submission.csv'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}